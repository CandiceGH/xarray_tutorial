{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xarray and parallel computing with dask\n",
    "\n",
    "xarray integrates with dask to support parallel computations and streaming computation on datasets that don’t fit into memory.\n",
    "\n",
    "### Outline\n",
    "- reading and writing data\n",
    "- automatic parallelization\n",
    "- dask schedulers and deployments\n",
    "\n",
    "### Tutorial Duriation\n",
    "10 minutes\n",
    "\n",
    "### Going Further\n",
    "\n",
    "- Xarray's Documentation on using dask: http://xarray.pydata.org/en/latest/dask.html\n",
    "- Dask's Documentation: http://dask.pydata.org/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask\n",
    "\n",
    "Dask is a flexible parallel computing library for analytic computing.\n",
    "\n",
    "Dask is composed of two components:\n",
    "\n",
    "- Dynamic task scheduling optimized for computation. This is similar to Airflow, Luigi, Celery, or Make, but optimized for interactive computational workloads.\n",
    "- “Big Data” collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of the dynamic task schedulers.\n",
    "\n",
    "![](images/collections-schedulers.png)\n",
    "\n",
    "\n",
    "Dask works by constructing \"task graphs\"\n",
    "\n",
    "![](images/transpose.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Writing data\n",
    "\n",
    "The usual way to create a dataset filled with dask arrays is to load the data from a netCDF file or files. You can do this by supplying a chunks argument to open_dataset() or using the open_mfdataset() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "np.warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dask's distributed scheduler for now\n",
    "# (see the last section of this notebook for more details)\n",
    "from distributed import Client, progress\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('./data/rasm.nc', chunks={'time': 10})  # this actually loads data using xr.open_dataset\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xarray is now using dask arrays under the hood\n",
    "ds['Tair'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Xarray operations using dask are now lazy\n",
    "t_range = (ds['Tair'].resample(time='AS').max('time') -\n",
    "           ds['Tair'].resample(time='AS').min('time')).mean('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_range.data.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = t_range.persist()\n",
    "progress(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic parallelization\n",
    "\n",
    "Almost all of xarray’s built-in operations work on dask arrays. If you want to use a function that isn’t wrapped by xarray, one option is to extract dask arrays from xarray objects (.data) and use dask directly.\n",
    "\n",
    "Another option is to use xarray’s apply_ufunc(), which can automate embarrassingly parallel “map” type operations where a functions written for processing NumPy arrays should be repeatedly applied to xarray objects containing dask arrays. It works similarly to `dask.array.map_blocks()` and `dask.array.atop()`, but without requiring an intermediate layer of abstraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bottleneck\n",
    "\n",
    "def covariance_gufunc(x, y):\n",
    "    return ((x - x.mean(axis=-1, keepdims=True))\n",
    "            * (y - y.mean(axis=-1, keepdims=True))).mean(axis=-1)\n",
    "\n",
    "def pearson_correlation_gufunc(x, y):\n",
    "    return covariance_gufunc(x, y) / (x.std(axis=-1) * y.std(axis=-1))\n",
    "\n",
    "def spearman_correlation_gufunc(x, y):\n",
    "    x_ranks = bottleneck.rankdata(x, axis=-1)\n",
    "    y_ranks = bottleneck.rankdata(y, axis=-1)\n",
    "    return pearson_correlation_gufunc(x_ranks, y_ranks)\n",
    "\n",
    "def spearman_correlation(x, y, dim):\n",
    "    return xr.apply_ufunc(\n",
    "        spearman_correlation_gufunc, x, y,\n",
    "        input_core_dims=[[dim], [dim]],\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['Tair'].roll(time=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.chunk({'x': 25, 'y': 25, 'time': -1})  # rechunk this dataset\n",
    "\n",
    "# create some toy data using our dataset\n",
    "x = ds['Tair']\n",
    "y = ds['Tair'].roll(time=1)  # lag the air temp data by one month\n",
    "y['time'] = x['time']  # reset the index \n",
    "\n",
    "\n",
    "# calculate the spearman correlation\n",
    "corr = spearman_correlation(x, y, 'time')\n",
    "corr = corr.where(corr < 1)  # make sure nans are masked properly\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dask schedulers and deployments\n",
    "\n",
    "Dask provides 4 different mechanisms for executing dask task graphs.\n",
    "\n",
    "- `dask.threaded.get`: a scheduler backed by a thread pool\n",
    "- `dask.multiprocessing.get`: a scheduler backed by a process pool\n",
    "- `dask.get`: a synchronous scheduler, good for debugging\n",
    "- `distributed.Client.get`: a distributed scheduler for executing graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.multiprocessing\n",
    "dask.config.set(scheduler='processes')  # overwrite default with multiprocessing scheduler\n",
    "\n",
    "# or\n",
    "\n",
    "from distributed import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lots more! \n",
    "\n",
    "- Setting up dask: http://dask.pydata.org/en/latest/setup.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pangeo)",
   "language": "python",
   "name": "pangeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
